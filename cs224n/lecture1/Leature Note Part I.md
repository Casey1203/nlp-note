# Leature Note: Part I

关键词：自然语言处理(NLP)，词向量，SVD，Skip-gram. 连续词袋（CBOW），负采样（Negative Sampling），层次（Hierarchical）Softmax，Word2Vec。

这一系列的文档通过引入NLP的概念和当今NLP面临的问题作为开端。之后我们会讨论通过数值向量来表示词的概念。最后我们会讨论一下设计词向量的流行方法。

## 1. NLP的介绍

首先我们来从宏观层面上来讨论什么是NLP。

### 1.1 NLP中最特别的是什么？

人类的自然语言中，最特别的是什么。人类语言是一个专门为了传递意思而搭建的系统。它不是像那些有实际形体的表示，因此NLP和计算机视觉或者是其他机器学习任务很不一样。

大多数词语对于语言外的实体而言只是一个符号：

例如，词汇“rocket”表示火箭的概念，也可以指火箭的实例。有一些例外，当我们用词语和字母来表达信号，例如"Whooompaa"。基于此，语言符号可以表示若干种形态：声音，手势，书写等等。它们以连续的信号传输到大脑中。

### 1.2 任务举例

在NLP中，从语音处理到语义解析和话语处理中，有许多不同级别的任务。NLP的任务是能够设计算法，从而让计算机“理解”自然语言，进而来执行一些任务。不同的任务，难度是不一样的。

简单难度：

拼写纠正、关键词检索、同义词检索。

中等难度：

从网站、文档中抽取信息等

困难难度：

机器翻译（例如：中英翻译）、语义分析（查询语句的含义）、指代问题（“他”和“它”在文档中指什么）、问答（例如：回答竞猜问题）。

### 1.3 词语的表达

在NLP任务中，最常见的是如何将词语表达，作为我们的模型输入。大多数早期的NLP工作中，我们都没有把单词看作是最小的原子符号。为了在大多数NLP任务中表现出色，我们首先需要能够描述单词的相似性与差异性。有了词向量，我们可以很简单地将这种能力编码进向量中（例如用Jaccard、Cosine、Euclidean距离等）。

## 2. 词向量

虽然在英语中，大约有1.3千万个词汇，但是它们之间难道都是没有联系的吗？例如“feline”和“cat”，“hotel”和“motel”？我认为它们不是没有联系。因此，我们想要将英语词汇编码成某种向量，使得每个词汇在“词汇空间”中代表了一个点。这有很多重要的原因，但是最直观的原因是，可能存在有一个N维的空间（N远小于1.3千万），可以充分地将我们的语言表达的所有意思都装进去。每个维度会装进一部分我们传递的信息。例如，语义可能会表达时态（过去式、现在式、将来式），数量（单数和复数）和性别（男性和女性）。

我们首先来看第一个词向量，也是最简单的一个：**独热向量（one-hot vector）**。将每个单词表达成$\mathbb{R}^{|V| \times 1}​$的向量。某个单词在英语单词的排序中的位置（index）为i，则这个单词的词向量，在第i个位置的元素值为1，其余的地方都为0。按照这个概念，$|V|​$就是我们单词表的单词数量。这种词向量，会将单词表达成以下形式。

![Image text](https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture1/img/onehotvec.png)

可以看到，我们将每个单词表示成了完全独立的个体。正如我们之前讨论到，这种单词的表达没有办法直接表达出单词相似性的概念。例如$(w^{hotel})^Tw^{motel}=(w^{hotel})^Tw^{cat}=0$。

因此我们尝试降低空间的维度，找到一个维度小于$\mathbb{R}^{|V| \times 1}$子空间，从而可以找到单词间的关系。

### 3. 基于奇异值分解（SVD）的方法

利用这种方法找到的词嵌入（也叫词向量），我们首先遍历整个数据集，将词语共同出现的次数累计起来，形成一个矩阵$X$。然后对矩阵$X$进行SVD分解，得到$USV^T$的分解形式。我们用矩阵$U$的行，作为单词的词嵌入。接下来讨论一下$X$矩阵的几种选择。

### 3.1 词-文档矩阵

我们做一个大胆的猜想，相关的词语会出现在同一篇文档中。例如，”银行“、”债券“、”股票“、“钱”等词语会出现在一起，而“银行”、“章鱼”，“香蕉”和“曲棍球”可能就不会一起出现。我们利用这个事实，来构造一个词-文档矩阵。做法是：循环检索成千上万的文档，对于每个出现在文档$j$中的单词$i$，我们把它加到$X_{ij}$中。显然这是一个很大的矩阵$\mathbb{R}^{|V| \times M}$，其中$M$表示文档的数量。也许我们可以做得比这个更好。

### 3.2 基于窗口的共现矩阵

和3.1同样的逻辑也被运用到这，矩阵$X$表示的是单词的共现矩阵，因此这是一个关联矩阵（affinity matrix）。这个方法中，我们计算，给定一个感兴趣的单词，在这个单词周围的一个特定大小的窗口内，每个单词出现的次数。我们计算预料中的所有单词出现的次数。下面展示一个例子。预料中只包含3个句子，窗口大小为1：

1. I enjoy flying.
2. I like NLP.
3. I like deep learning.

计数矩阵的结果是

![Image text](https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture1/img/countmat.png)

### 3.3 将SVD方法运用到共现矩阵上

在矩阵$X$上运用SVD分解，观察奇异值（分解后的$S$矩阵的对角线）。基于期望解释的方差百分比$\frac{\sum_{i=1}^k{\sigma_i}}{\sum_{i=1}^{|V|}{\sigma_i}}$，选择在某个位置$k$截断。我们认为子矩阵$U_{1:|V|,1:k}$作为我们的词嵌入矩阵。因此这就是单词表中的每个单词的$k$维表示。

下面是在共现矩阵$X$上使用SVD分解的过程

![Image text](https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture1/img/svdx.png)

通过选择前$k$个奇异值对应的奇异向量，达到降维的效果。

