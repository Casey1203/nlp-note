# Leature Note: Part I

关键词：自然语言处理(NLP)，词向量，SVD，Skip-gram. 连续词袋（CBOW），负采样（Negative Sampling），层次（Hierarchical）Softmax，Word2Vec。

这一系列的文档通过引入NLP的概念和当今NLP面临的问题作为开端。之后我们会讨论通过数值向量来表示词的概念。最后我们会讨论一下设计词向量的流行方法。

## 1. NLP的介绍

首先我们来从宏观层面上来讨论什么是NLP。

### 1.1 NLP中最特别的是什么？

人类的自然语言中，最特别的是什么。人类语言是一个专门为了传递意思而搭建的系统。它不是像那些有实际形体的表示，因此NLP和计算机视觉或者是其他机器学习任务很不一样。

大多数词语对于语言外的实体而言只是一个符号：

例如，词汇“rocket”表示火箭的概念，也可以指火箭的实例。有一些例外，当我们用词语和字母来表达信号，例如"Whooompaa"。基于此，语言符号可以表示若干种形态：声音，手势，书写等等。它们以连续的信号传输到大脑中。

### 1.2 任务举例

在NLP中，从语音处理到语义解析和话语处理中，有许多不同级别的任务。NLP的任务是能够设计算法，从而让计算机“理解”自然语言，进而来执行一些任务。不同的任务，难度是不一样的。

简单难度：

拼写纠正、关键词检索、同义词检索。

中等难度：

从网站、文档中抽取信息等

困难难度：

机器翻译（例如：中英翻译）、语义分析（查询语句的含义）、指代问题（“他”和“它”在文档中指什么）、问答（例如：回答竞猜问题）。

### 1.3 词语的表达

在NLP任务中，最常见的是如何将词语表达，作为我们的模型输入。大多数早期的NLP工作中，我们都没有把单词看作是最小的原子符号。为了在大多数NLP任务中表现出色，我们首先需要能够描述单词的相似性与差异性。有了词向量，我们可以很简单地将这种能力编码进向量中（例如用Jaccard、Cosine、Euclidean距离等）。

## 2. 词向量

虽然在英语中，大约有1.3千万个词汇，但是它们之间难道都是没有联系的吗？例如“feline”和“cat”，“hotel”和“motel”？我认为它们不是没有联系。因此，我们想要将英语词汇编码成某种向量，使得每个词汇在“词汇空间”中代表了一个点。这有很多重要的原因，但是最直观的原因是，可能存在有一个N维的空间（N远小于1.3千万），可以充分地将我们的语言表达的所有意思都装进去。每个维度会装进一部分我们传递的信息。例如，语义可能会表达时态（过去式、现在式、将来式），数量（单数和复数）和性别（男性和女性）。

我们首先来看第一个词向量，也是最简单的一个：**独热向量（one-hot vector）**。将每个单词表达成$\mathbb{R}^{|V|} \times 1$的向量。某个单词在英语单词的排序中的位置（index）为i，则这个单词的词向量，在第i个位置的元素值为1，其余的地方都为0。按照这个概念，$|V|$就是我们单词表的单词数量。这种词向量，会将单词表达成以下形式。

