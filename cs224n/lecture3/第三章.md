# 第三章

关键词：神经网络。前向传播计算。后向传播计算。神经元。最大边界损失。梯度检查。Xavier参数初始化法。学习率。Adagrad。

该笔记介绍了单层和多层的神经网络，以及它们如何用来做分类任务。然后讨论了利用被称作反向传播法的分布梯度下降法来训练网络。我们将会看到链式法则如何用来更新参数。在严谨的数学讨论之后，我们将会探讨一些在实践中训练神经网络的小技巧，这包括，（非线形）神经元、梯度检查、Xavier参数初始化法、学习率、Adagrad等。最后，我们会引出利用循环神经网络来做语言模型。



## 1. 神经网络：基础

基于之前的讨论，许多数据无法线形可分，导致效果有限，因此需要非线性的分类器。神经网络是拥有非线性决策边界的分类起家族，如图1。现在我们知道了神经网络可以产生怎样的决策边界，接下来看看它是怎么实现的。

### 1 神经元

神经元是一个广义的计算单元，输入$n$个变量，输出1个结果。通过控制参数（也叫做权重）来控制输出的不同。最有名的神经元是“sigmoid”或"二元logistic回归"单元。这个单元需要n维的输入向量$x$，产生一个标量激活值$a$。这个神经元通常和n维权重向量和1维偏置标量联系在一起。其输出可以用下式表示
$$
a=\frac{1}{1+\exp(-w^Tx+b)}
$$
我们也可以将$w$和$b$写在一起，等价于下式。
$$
a=\frac{1}{1+\exp(-[w^T \quad b]\cdot [x \quad 1])}
$$
这个式子可以可视化成图2的样子。



### 1.2 单层神经元

接着上面的话题，我们扩展到多个神经元，将输入$x$作为许多个这样的神经元的输入，如图3。

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure1.jpg" width = "200" height = "250" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure2.png" width = "250" height = "250" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure3.png" width = "250" height = "250" div/>
</figure>

如果我们把不同的神经元用权重$\{w^{(1)},\cdots, w^{(m)}\}$和偏置$\{b_1,\cdots,b_m\}$来表达，可以将每个神经元的激活值表示成
$$
\begin{align*}
a_1 &= \frac{1}{1 + \exp(w^{(1)T}x + b_1)}\\
\vdots \\
a_m &= \frac{1}{1 + \exp(w^{(m)T}x + b_m)}
\end{align*}
$$
为了简化表达，我们用下面的缩写来替代，进而能够更加方便的表达复杂的网络。
$$
\begin{align*}
\sigma(z) &= \begin{bmatrix}  \frac{1}{1 + \exp(z_1)}\\ \vdots \\  \frac{1}{1 + \exp(z_m)}\end{bmatrix} = \begin{bmatrix} a_1 \\ \vdots \\ a_m\end{bmatrix}
\\
b &= \begin{bmatrix}  b_1\\ \vdots \\  b_m \end{bmatrix} \in \mathbb{R}^{m}\\
W &= \begin{bmatrix} - & w^{(1)T} & -\\ & \cdots & \\ - & w^{(m)T} & - \end{bmatrix} \in \mathbb{R}^{m\times n}
\end{align*}
$$
我们可以将输出写成
$$
z=Wx+b
$$
sigmoid函数输出的激活值可以写成
$$
\begin{bmatrix}  a^{(1)}\\ \vdots \\  a^{(m)} \end{bmatrix} = \sigma (z) = \sigma (Wx + b)
$$
那么，这些激活值想要告诉我们什么？一种说法是，这些激活值可以作为一些指标，标明是否出现了一些特定的特征的加权组合。我们可以使用这些激活值的组合来做一些分类任务。

### 1.3 前向传播计算

我们已经看到了输入向量$x\in \mathbb{R}^n$是怎样输入到一层sigmoid单元，并且产生激活向量$a\in \mathbb{R}^m$。但是背后的原理的什么？考虑接下来的在NLP中的命名实体识别任务作为例子。
$$
\textit{"Museums in Paris are amazing"}
$$
我们想要将中心词$\textit{"Paris"}$分类，判断它是否是一个实体。在这样的例子中，我们不仅仅想要判断在词向量窗口中是否出现了这个词，我们还要将其和窗口中其他的词语之间的关系捕捉到。例如，当$\textit{Museums}$是第一个词，并且只有$\textit{in}$是第二个词时。直接将输入送到softmax函数中，没办法得到这样的非线性决策。相反，我们需要依赖在1.2节介绍的中间层。我们使用另外一个矩阵$U \in \mathbb{R}^{m\times 1}$，利用激活值，来产生一个没有被归一化的分数。
$$
s = U^Ta = U^T f(Wx + b)
$$
其中$f$是激活函数。

**维度分析**：如果我们将每个单词，用4维的词向量表示，并且使用5个单词形成的窗口作为输入（和上面的例子一样），这样输入就是$x\in \mathbb{R}^{20}$。如果我们在隐藏层中使用8个sigmoid单元，并且利用了激活函数产生了1个输出得分，这样，$W \in \mathbb{R}^{8\times 20}$, $b \in \mathbb{R}^{8}$, $U \in \mathbb{R}^{8\times 1}$, $s \in \mathbb{R}$。

如图4，可以看到矩阵和向量中的每个元素，在网络中对应的位置和含义。

### 1.4 最大的边界的目标函数

和大多数机器学习模型一样，神经网络也需要一个优化目标，一个衡量错误或者好坏的指标，进而我们可以去最小化或是最大化。我们会讨论一个流行的错误指标，称为最大的边界目标。使用这个目标的想法是为了保证，"真"的数据的得分要比"假"的数据的得分要高。

利用之前的例子，我们把$\textit{"Museums in Paris are amazing"}$这句话标称"真"的得分记做$s$，并且把$ \textit{"Not all museums in Paris"}$标记成"假"的分数记做$s_c$。（下标$c$表示这个窗口中的数据是不正常的）

我们的目标函数是最大化$(s-s_c)$或者是最小化$(s_c-s)$。然而，我们修改目标，仅仅在$s_c > s \Rightarrow (s_c - s) > 0$的情况下算作出错。这个原理是因为，我们仅仅关心那些"真"的数据的得分要高于"假"的数据的得分，其余的都不考虑。因此，我们希望，当$s_c>s$时，误差为$s_c-s$，而反过来误差为0。因此，我们的优化目标是
$$
\text{minimize} \ J = \max(s_c - s, 0)
$$
然而，上面的优化目标，在某种程度上，还不是很有把握的产生一个安全的边界。我们希望"真"的数据的得分比"假"的数据的得分要高出一定的范围。换句话说，我们希望当$(s-s_c < \Delta)$时，应该算作误差，而不仅仅是当$(s-s_c < 0)$。因此，我们修改优化目标为
$$
\text{minimize} \ J = \max(\Delta + s_c - s, 0)
$$
我们把边界放缩一下，使得$\Delta=1$，并且使的其他优化中的参数不会影响到表现。更多的信息，可以查阅函数间隔与几何间隔的话题，通常在讲支撑向量机中会提到。最后，在所有训练的窗口中，我们定义以下优化目标。
$$
\text{minimize} \ J = \max(1 + s_c - s, 0)
$$
上式中，$s_c = U^T f(Wx_c + b)$， $s = U^T f(Wx + b)$。

### 1.5 后向传播的训练方法：基础

在本节，我们讨论，当1.4节中的损失函数$J$大于0时，如何训练模型中不同的参数。当损失等于0时，不需要更新任何的参数。因此，我们使用梯度下降法（或者是SGD）来更新参数。我们需要所有参数的梯度信息作为更新参数的条件，如下式
$$
\theta^{(t+1)}=\theta^{(t)}-\alpha \nabla_{\theta^{(t)}} J
$$
后向传播通过使用微分的链式法则，来计算损失函数对每个前向传播中的参数的梯度的方法。为了更加深入的理解，我们采用图5中一个基本的网络结构来运行后向传播方法。

我们使用只有一层隐藏层和一个输出单元的神经网络。为了能够更好的表达网络，我们先给出一些定义。

- $x_i$是输入到神经网络的一个输入向量。
- $s$是神经网络的输出。
- 每层（包括输入和输出层）都有神经元会接收到输入，并且产生输出。第$k$层中的第$j$个神经元会接收到输入中的标量$z_j^{(k)}$，并且产生激活值$a_j^{(k)}$，这是标量。
- 我们将在$z_j^{(k)}$位置上计算的后向传播误差称之为$\delta_j^{(k)}$。
- 第一层表示输入层，而不是第一个隐藏层。因此，对于输入层而言，$x_{j}=z_{j}^{(1)}=a_{j}^{(1)}$。
- $W^{(k)}$是一个转移矩阵，将来自第$k$层的输出转移到第$k+1$层的输入。因此我们用以下的定义，将1.3小节中提到的矩阵表示成$W^{(1)}=W$（layer1->layer2的转移矩阵），$W^{(2)}=U$。



<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure4.png" width = "250" height = "300" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure5.png" width = "280" height = "300" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure6.png" width = "250" height = "250" div/>
</figure>



那我们开始吧。假设损失函数是$J=(1+s_c-s)$是一个正数（$J=\max(1+s_c-s,0)>0$），并且我们希望更新参数$W_{14}^{(1)}$（如图5和图6显示，$W_{14}^{(1)}$是layer1和layer2之间的权重，从layer2的neuron1指回layer1的neuron4）。我们发现$W_{14}^{(1)}$仅仅和$z_1^{(2)}$进而是$a_1^{(2)}$有关。这是一个理解后向传播很重要的点，即参数的后向传播梯度仅仅受到该参数有贡献的值的影响。这里，$a_1^{(2)}$是在计算得分的前向传播过程中，和$W_1^{(2)}$相乘。我们可以从最大的边界损失中可以看出。
$$
\frac{\partial J}{\partial s}=-\frac{\partial J}{\partial s_{c}}=-1
$$
因此，我们可以忽略从$J$到$s$的导数，直接考虑从$s$到$W$的导数$\frac{\partial s}{\partial W_{ij}^{(1)}}$，因此
$$
\begin{align*}
\frac{\partial s}{\partial W^{(1)}_{ij}} &= \frac{\partial W^{(2)} a^{(2)}}{\partial W^{(1)}_{ij}} = \frac{\partial W^{(2)}_{i} a^{(2)}_i}{\partial W^{(1)}_{ij}} = W^{(2)}_{i} \frac{\partial a^{(2)}_i}{\partial W^{(1)}_{ij}}\\
\Rightarrow W^{(2)}_{i} \frac{\partial a^{(2)}_i}{\partial W^{(1)}_{ij}} &=  W^{(2)}_{i} \frac{\partial a^{(2)}_i}{\partial z^{(2)}_i} \frac{\partial z^{(2)}_i}{\partial W^{(1)}_{ij}}\\
&=  W^{(2)}_{i} \frac{f(z^{(2)}_i)}{\partial z^{(2)}_i} \frac{\partial z^{(2)}_i}{\partial W^{(1)}_{ij}}\\
&=  W^{(2)}_{i} f'(z^{(2)}_i) \frac{\partial z^{(2)}_i}{\partial W^{(1)}_{ij}}\\
&=  W^{(2)}_{i} f'(z^{(2)}_i) \frac{\partial}{\partial W^{(1)}_{ij}} (b^{(1)}_i + a_1^{(1)}  W^{(1)}_{i1} + a_2^{(1)}  W^{(1)}_{i2} + a_3^{(1)}  W^{(1)}_{i3} + a_4^{(1)}  W^{(1)}_{i4})\\
&=  W^{(2)}_{i} f'(z^{(2)}_i) \frac{\partial}{\partial W^{(1)}_{ij}} (b^{(1)}_i + \sum_k a_k^{(1)}  W^{(1)}_{ik})\\
&=  W^{(2)}_{i} f'(z^{(2)}_i) a_j^{(1)}\\
&=  \delta_i^{(2)} \cdot a_j^{(1)}
\end{align*}
$$
这一段比较复杂，我们一步一步来看。

现在的目标是求得分$s$对参数$W^{(1)}$的偏导数。
$$
\begin{align*}
\frac{\partial s}{\partial W^{(1)}}&=\frac{\partial}{\partial W^{(1)}}(W^{(2)})^Ta^{(2)} \\
&=\frac{\partial}{\partial W^{(1)}}(W^{(2)})^Tf(z^{(2)})\\
&=\frac{\partial}{\partial W^{(1)}}(W^{(2)})^Tf(W^{(1)}x+b)
\end{align*}
$$
可以看到$W^{(1)}$出现在了激活函数中，与输入$x$有关系。

以$W_{ij}^{(1)}$为例，$W_{ij}^{(1)}$是layer2的第$i$个神经元指回layer1的第$j$个神经元的权重，因此只与layer2的第$i$个神经元有关，因此只会影响$a^{(2)}_i$。所以
$$
\begin{align}
\frac{\partial}{\partial W_{ij}^{(1)}}(W^{(2)})^Tf(W^{(1)}x+b) & =(W^{(2)})^T\frac{\partial}{\partial W_{ij}^{(1)}} f(W^{(1)}x+b) \\
&= (W^{(2)})^T f'(W^{(1)}x+b) \frac{\partial}{\partial W_{ij}^{(1)}}(W^{(1)}x+b) \\
&=(W^{(2)})^T f'(W^{(1)}x+b) \frac{\partial}{\partial W_{ij}^{(1)}}  \left(
\begin{bmatrix}  W^{(1)}_{11} \cdots W^{(1)}_{1n} \\ \vdots \\   W^{(1)}_{m1} \cdots W^{(1)}_{mn} \end{bmatrix}\begin{bmatrix} x_1 \\\vdots\\ x_n\end{bmatrix} + \begin{bmatrix} b_1 \\\vdots\\ b_n\end{bmatrix} \right) \\
&=(W^{(2)})^T f'(W^{(1)}x+b) \frac{\partial}{\partial W_{ij}^{(1)}}  \left( \begin{bmatrix} W_{11}^{(1)}x_1+\cdots + W_{1n}^{(1)}x_n + b_1 \\ \vdots \\W_{m1}^{(1)}x_1+\cdots + W_{mn}^{(1)}x_n + b_n \end{bmatrix} \right) \\
&= (W^{(2)})^T f'(W^{(1)}x+b)x_j
\end{align}
$$
