# 第三章第2部分

## 2. 神经网络：技巧

在讨论了神经网络背后的数学基础后，我们现在要探讨实践中神经网络的使用技巧。

### 2.1 梯度校验

在上一节，我们利用了基于微积分的解析法，仔细讨论了怎样计算了误差关于模型参数的梯度，以及如何更新参数。现在我们要介绍一种数值的方法来近似计算这些梯度，尽管这种方法在训练网络中效率很低，但它让我们非常精确地估计了每个参数的梯度。因此它可以作为我们之前用解析法进行求梯度是否正确。一个模型的参数向量$\theta$和损失函数$J$，利用**centered difference formula**方法数值法计算在$\theta_i$附近的梯度
$$
f^{\prime}(\theta) \approx \frac{J\left(\theta^{(i+)}\right)-J\left(\theta^{(i-)}\right)}{2 \epsilon}
$$
其中$\epsilon$是一个很小的数（通常为$1e^{-5}$）。$J(\theta^{(i+)})$项是将参数$\theta$的第$i$个元素增加$\epsilon$，通过前向传播算法计算出的误差。相似的，$J(\theta^{(i-)})$项是将参数$\theta$的第$i$个元素减少$\epsilon$，通过前向传播算法计算出的误差。因此，使用两次前向传播，我们可以估计出误差对模型中任何参数的梯度。注意，数值型梯度的概念本应该和导数的概念一致，为
$$
f^{\prime}(x) \approx \frac{f(x+\epsilon)-f(x)}{\epsilon}
$$
这和之前的做法有一点不同，因为这仅仅是将$x$往正的方向扰动了$\epsilon$来计算梯度。尽管这种方法也可以接受，但实际情况，还是使用centered difference formula方法，通过扰动两个方向，结果更加精确和稳定。理由是，为了更好的把函数$f$在一个点附近的梯度/斜率给估计出来，我们需要检查$f$函数在这个点的左边和右边的性质。使用泰勒定理，可以知道利用centered difference formula计算出的梯度的误差大概是$\epsilon^2$级别的，这是一个非常小的数字。相反，普通的计算导数的方法会更容易产生误差。

现在，你可能会有一个问题，既然这个方法很精确，为什么我们在计算网络的梯度时，不采用这种方法，而要采用后向传播法呢。答案很简单，之前也提到过，是因为这种方法太耗时间了。试想每次我们要计算关于一个元素的梯度时，我们需要做两次前向传播，这很浪费计算。而且，许多大型的神经网络会包含成千上万的参数，对每个参数都计算两次前向传播的方法不是最优的。并且，在优化方法中，例如SGD，我们每次迭代都要计算很多梯度，这样的迭代有几千次，显然这种方法会变得难以控制。因为这非常美效率，所以我们仅使用它来进行解析法的梯度校验，这样计算才比较快。一个标准的梯度校验的方法是如下的代码



### 2.2 正则化

和许多机器学习模型一样，神经网络非常容易过拟合，即它会在训练集上获得了完美的表现，但是却在没见过的数据上泛化能力比较差。一个常用的压制过拟合（也叫做“高方差问题”）方法是引入L2正则惩罚。方法是在损失函数$J$的后面添加一项，因此计算整体的损失为
$$
J_{R}=J+\lambda \sum_{i=1}^{L}\left\|W^{(i)}\right\|_{F}
$$
在上面的函数中，$\left\|W^{(i)}\right\|_{F}$是矩阵$W^{(i)}$（网络中第$i$个权重矩阵）的Frobenius范数，$\lambda$是超参数，用于控制正则项相对于原来的损失函数的比重。这相当于当我们在优化原始的损失函数，即我们试图最小化$J_R$时，正则化在惩罚过大的权重。由于Frobenius范数的二次现象（即要计算矩阵中每个元素的平方之和），L2正则化可以有效的减少模型的灵活度，因此会减少过拟合的现象。引入这样的约束可以解释为，我们先验性地认为最优的权重应该是接近0的。至于如何接近，这取决于$\lambda$。正确地选择$\lambda$很重要，需要通过超参数调参。$\lambda$太大会导致权重都接近0，模型就无法从训练集学到任何有意义的信息，这会导致模型在训练集、验证集和测试集上有糟糕的准确率。如果$\lambda$太小，我们又会陷入过拟合的领域中。注意，偏置项不需要被正则化，从而不需要影响损失函数，思考为什么偏置项不需要。

的确，有时也会使用其他类型的正则化方法，例如L1正则化，即把矩阵中的每个元素取绝对值后，再加和起来（而不是平方和）。然而，实际中它比较邵雍，因为它会导致参数的稀疏。再下一节中，我们会讨论dropout，这是另外一种有效的正则化方法，它通过随机将一些神经元，在前向传播中的连接打断（例如将对应的权重矩阵中的元素设置乘0）。

### 2.3 Ｄropout

Dropout是一个很强大的正则化方法，它第一次被提到是在Dropout: A Simple Way to Prevent Neural Net-works from Overfitting.这篇论文中。它的想法很简单，但是很有效，即，在进行前向、后向传播的训练过程中，我们会以$(1-p)$的概率，随机的“丢失”一些神经元（等价于，我们会让每个神经元以$p$的概率存活）。然后，在测试阶段，我们会用完整的网络来计算我们关于样本的预测值。通过Dropout，网络一般会从数据集中学到更加有意义的信息，过拟合的可能性更小，因此通常能够获得在指定任务中更好的表现。为什么Dropout很有效，一个直觉的原因是，Dropout能够使得每次训练的网络呈现指数级别的小，然后在预测的时候，将这些网络平均起来。

实践中，我们引入Dropout的方法是，把每层的神经元的输出$h$，按照概率为$p$的方式将其保持不变，而以$(1-p)$的概率将其设置为0。然后，在后向传播过程中，我们仅仅将梯度在那些存活的神经元中通过。最后，在测试阶段，我们在前向传播中，保持所有的神经元都是存活的。但是，一个关键的问题是，为了让Dropout能够有效地起作用，在测试过程中，神经元的输出的期望值应该要和训练过程中的大概保持一致，否则，这两个阶段，神经元的输出将会是不同的量级，则网络的表现不再是确定的。因此，我们要在测试阶段，将每个神经元的输出，除以一个特定的数。如何确定这个数的值，使得在训练和测试阶段，神经元输出的值的期望能够等加，这个问题留给读者。

### 2.4 神经单元

到此为止，我们已经讨论了包含sigmoid神经元，从而带来了非线性特点的神经网络。然而，在许多应用中，使用其他的激活函数，网络能够表现得更好。下面列出了一些常见的做法，包含了函数以及对应的梯度定义，用于替代我们之前讨论的sigmoid函数。

**Sigmoid：**函数形状如图9，这是我们讨论过的一种默认的选择，激活函数的形式如下
$$
\sigma(z)=\frac{1}{1+\exp (-z)}
$$
其中$\sigma(z)\in (0,1)$。它的梯度是
$$
\sigma^{\prime}(z)=\frac{-\exp (-z)}{1+\exp (-z)}=\sigma(z)(1-\sigma(z))
$$
**Tanh：**函数形状如图10，Tanh函数可以替代Sigmoid函数，实践中，它通常能够更快速的收敛。它和Sigmoid函数之间主要的区别在于，Tanh函数输出的范围是-1到1，而Sigmoid函数的范围是0到1。
$$
\tanh (z)=\frac{\exp (z)-\exp (-z)}{\exp (z)+\exp (-z)}=2 \sigma(2 z)-1
$$
其中，$\tanh (z) \in(-1,1)$。它的梯度是
$$
\tanh ^{\prime}(z)=1-\left(\frac{\exp (z)-\exp (-z)}{\exp (z)+\exp (-z)}\right)^{2}=1-\tanh ^{2}(z)
$$

<figure class="half">
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure9.jpg" width = "200" height = "200" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure10.png" width = "250" height = "250" div/>
  <img src="https://raw.githubusercontent.com/Casey1203/nlp-note/master/cs224n/lecture3/img/figure11.png" width = "250" height = "250" div/>
</figure>
**Hard tanh：**函数形状如图11。hard tanh函数有的时候比tanh函数更加受欢迎，原因是因为它计算效率快。但是，它在$z$大于1的地方，函数会陷入平坦，它的激活函数是
$$
\operatorname{hardtanh}(z)=\left\{\begin{array}{cc}{-1} & { : z<-1} \\ {z} & { :-1 \leq z \leq 1} \\ {1} & { : z>1}\end{array}\right.
$$
它的导数也可以用分段函数来表达
$$
\operatorname{hardtanh}^{\prime}(z)=\left\{\begin{array}{ll}{1} & { :-1 \leq z \leq 1} \\ {0} & { : \text { otherwise }}\end{array}\right.
$$
**Soft sign：**函数形状如图12。soft sign函数是另外一种非线性函数，也可以被视为tanh函数的替代，因为它不会像强行截断的函数那样容易陷入平坦，它的激活函数是
$$
\operatorname{softsign}(z)=\frac{z}{1+|z|}
$$
它的导数是
$$
\operatorname{softsign}^{\prime}(z)=\frac{\operatorname{sgn}(z)}{(1+z)^{2}}
$$
其中$\operatorname{sgn}$函数是符号函数，根据$z$的符号，返回$\pm 1$。

**ReLU：**函数形状如图13。ReLU（线性整流单元Rectified Linear Unit）函数是一个受欢迎的激活函数，因为即使$z$很大，它也不会陷入平坦，在计算机视觉领域已经有了很成功的案例。
$$
\operatorname{rect}(z)=\max (z, 0)
$$
它的导数可以用分段函数表示
$$
\operatorname{rect}^{\prime}(z)=\left\{\begin{array}{ll}{1} & { : z>0} \\ {0} & { : \text { otherwise }}\end{array}\right.
$$
**Leaky ReLU：**函数形状如图14。传统的ReLU单元对于不是正数的$z$，不会传播误差。leaky ReLU激活函数修改了这个问题，当$z$是负数时，它仍会后向传播一个小的误差。
$$
\begin{array}{c}{\operatorname{leaky}(z)=\max (z, k \cdot z)} \\ {\quad \text { where } 0<k<1}\end{array}
$$
它的导数是
$$
\operatorname{leaky}^{\prime}(z)=\left\{\begin{array}{ll}{1} & { : z>0} \\ {k} & { : \text { otherwise }}\end{array}\right.
$$

### 2.5 数据预处理

与通常的机器学习模型一样，模型能够在任务中取得合理的表现的一个关键步骤，是数据预处理。以下介绍一些常见的预处理技术。

**去均值化**

给定一组输入数据集$X$，我们习惯于把数据集$X$减去它的特征向量的平均值，将数据给去中心化。实践中一个很重要的一点是，这里的平均值，仅仅是通过训练集计算的，并且这个均值将被用于训练集、验证集和测试集中。

**归一化**

另外一个常见的做法（即使可能没有比去均值化那么常见）是把输入数据的每个特征，放缩到一个相似的量级范围内。这种方法很有用，因为输入的特征通常有不同的单位，但是我们想要让所有的特征都同等的重要。为此，我们简单地除以在训练集中每个特征各自的标准差。

**白化**

白化不像去均值和归一化那么常见，它使得数据集的协方差矩阵是一个单位矩阵，这意味着特征之间是不相关的，并且每个特征的方差等于1。