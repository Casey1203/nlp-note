# 第三章第2部分

补充：关于softmax函数的求导

已知softmax函数的表达式为
$$
\begin{align}
\text{softmax}(x)&=\begin{bmatrix}\text{softmax}_1(x), \cdots, \text{softmax}_n(x)\end{bmatrix} \\
&= \begin{bmatrix} \frac{e^{x_1}}{\sum_m{e^{x_m}}} \cdots, \frac{e^{x_n}}{\sum_m{e^{x_m}}} \end{bmatrix}
\end{align}
$$
对于一个softmax神经元而言，它接受输入$z$，给出输出$a=\text{softmax}(z)$，把向量展开得到
$$
\begin{bmatrix}a_1,\cdots, a_n\end{bmatrix}=\begin{bmatrix}\text{softmax}_1(z),\cdots, \text{softmax}_n(z)\end{bmatrix}
$$
所以softmax的导数
$$
\frac{\partial a}{\partial z}=\frac{\partial}{\partial z} \text{softmax}(z)
$$
这里是向量对向量的偏导数，可以得到$\mathbb{R}^{n\times n}$的雅可比矩阵。矩阵的第$i$行第$j$列为
$$
\begin{align}
\frac{\partial a_i} {\partial z_j} & =\frac{\partial}{\partial z_j} \text{softmax}_i(z) \\
&= \frac{\partial}{\partial z_j} \frac{e^{z_i}}{\sum_{m}e^{z_m}}
\end{align}
$$
当$i=j$时，
$$
\begin{align}
\frac{\partial a_i} {\partial z_j}&=\frac{e^{z_i}\sum_m e^{z_m} - e^{z_i}e^{z_j}}{(\sum_{m}e^{z_m})^2} \\
&=\text{softmax}_i(z)-\text{softmax}_i(z)\cdot \text{softmax}_j(z) \\
&=\text{softmax}_i(z)-\text{softmax}_i(z)\cdot \text{softmax}_i(z)
\end{align}
$$
当$i\neq j$时，
$$
\begin{align}
\frac{\partial a_i} {\partial z_j}&=\frac{0\cdot\sum_m e^{z_m} - e^{z_i}e^{z_j}}{(\sum_{m}e^{z_m})^2} \\
&=-\text{softmax}_i(z)\cdot \text{softmax}_j(z)
\end{align}
$$
写成向量形式
$$
\frac{\partial a}{\partial z}=\begin{bmatrix} \frac{\partial a_1}{\partial z_1} \cdots \frac{\partial a_1}{\partial z_n} \\ \vdots \\ \frac{\partial a_n}{\partial z_1} \cdots \frac{\partial a_n}{\partial z_n} \end{bmatrix}
$$
实现方法，参考https://stackoverflow.com/questions/54976533/derivative-of-softmax-function-in-python

