# 第三章第2部分

## 2. 神经网络：技巧

在讨论了神经网络背后的数学基础后，我们现在要探讨实践中神经网络的使用技巧。

### 2.1 梯度校验

在上一节，我们利用了基于微积分的解析法，仔细讨论了怎样计算了误差关于模型参数的梯度，以及如何更新参数。现在我们要介绍一种数值的方法来近似计算这些梯度，尽管这种方法在训练网络中效率很低，但它让我们非常精确地估计了每个参数的梯度。因此它可以作为我们之前用解析法进行求梯度是否正确。一个模型的参数向量$\theta$和损失函数$J$，利用**centered difference formula**方法数值法计算在$\theta_i$附近的梯度
$$
f^{\prime}(\theta) \approx \frac{J\left(\theta^{(i+)}\right)-J\left(\theta^{(i-)}\right)}{2 \epsilon}
$$
其中$\epsilon$是一个很小的数（通常为$1e^{-5}$）。$J(\theta^{(i+)})$项是将参数$\theta$的第$i$个元素增加$\epsilon$，通过前向传播算法计算出的误差。相似的，$J(\theta^{(i-)})$项是将参数$\theta$的第$i$个元素减少$\epsilon$，通过前向传播算法计算出的误差。因此，使用两次前向传播，我们可以估计出误差对模型中任何参数的梯度。注意，数值型梯度的概念本应该和导数的概念一致，为
$$
f^{\prime}(x) \approx \frac{f(x+\epsilon)-f(x)}{\epsilon}
$$
这和之前的做法有一点不同，因为这仅仅是将$x$往正的方向扰动了$\epsilon$来计算梯度。尽管这种方法也可以接受，但实际情况，还是使用centered difference formula方法，通过扰动两个方向，结果更加精确和稳定。理由是，为了更好的把函数$f$在一个点附近的梯度/斜率给估计出来，我们需要检查$f$函数在这个点的左边和右边的性质。使用泰勒定理，可以知道利用centered difference formula计算出的梯度的误差大概是$\epsilon^2$级别的，这是一个非常小的数字。相反，普通的计算导数的方法会更容易产生误差。

现在，你可能会有一个问题，既然这个方法很精确，为什么我们在计算网络的梯度时，不采用这种方法，而要采用后向传播法呢。答案很简单，之前也提到过，是因为这种方法太耗时间了。试想每次我们要计算关于一个元素的梯度时，我们需要做两次前向传播，这很浪费计算。而且，许多大型的神经网络会包含成千上万的参数，对每个参数都计算两次前向传播的方法不是最优的。并且，在优化方法中，例如SGD，我们每次迭代都要计算很多梯度，这样的迭代有几千次，显然这种方法会变得难以控制。因为这非常美效率，所以我们仅使用它来进行解析法的梯度校验，这样计算才比较快。一个标准的梯度校验的方法是如下的代码



### 2.2 正则化

和许多机器学习模型一样，神经网络非常容易过拟合，即它会在训练集上获得了完美的表现，但是却在没见过的数据上泛化能力比较差。一个常用的压制过拟合（也叫做“高方差问题”）方法是引入L2正则惩罚。方法是在损失函数$J$的后面添加一项，因此计算整体的损失为
$$
J_{R}=J+\lambda \sum_{i=1}^{L}\left\|W^{(i)}\right\|_{F}
$$
在上面的函数中，$\left\|W^{(i)}\right\|_{F}$是矩阵$W^{(i)}$（网络中第$i$个权重矩阵）的Frobenius范数，$\lambda$是超参数，用于控制正则项相对于原来的损失函数的比重。因为我们试图最小化$J_R$

